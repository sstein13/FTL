{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import copy\n",
    "import holidays\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=aahssdbods.amfam.com;DATABASE=OperationalDataStore;Trusted_Connection=yes')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definitions\n",
    "today = dt.date.today()\n",
    "current_year = str(dt.date.today())[:4]\n",
    "weekday_dict = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\n",
    "moving_holidays = [\"New Year's Day\", \"Juneteenth National Independence Day\", \"Independence Day\", \"Veterans Day\", \"Christmas Day\"]\n",
    "static_holidays= [\"Martin Luther King Jr. Day\", \"Memorial Day\", \"Labor Day\", \"Columbus Day\", \"Thanksgiving\"]\n",
    "data_dict = None\n",
    "data_date = None\n",
    "data_opco = None\n",
    "data_unit = None\n",
    "yearly_volume_dict = {}\n",
    "master_volume_dict = {}\n",
    "queries_saved = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe from the Actual Volume excel file and cleans data\n",
    "CAH_volume_df = pd.read_excel(\"Actual Volume.xlsx\")\n",
    "CAH_volume_df.fillna(0, inplace = True)\n",
    "CAH_volume_df.rename(columns={\"# Service Level Calls Offered\":\"Date\", \"Unnamed: 1\":\"Start of Week\", \"Unnamed: 2\":\"Day of Week\", \"Department\":\"Advisor\", \"Unnamed: 4\":\"Agency\", \"Unnamed: 5\":\"Agency Helpline\", \"Unnamed: 6\":\"ASU\", \"Unnamed: 7\":\"ASU Set\", \"Unnamed: 8\":\"Claims Back Office\", \"Unnamed: 9\":\"Claims Hertz\", \"Unnamed: 10\":\"Claims Lead Line\", \"Unnamed: 11\":\"Claims Material Damage\", \"Unnamed: 12\":\"Claims Service Center\", \"Unnamed: 13\":\"Claims Team Lead\", \"Unnamed: 14\":\"Client Service\", \"Unnamed: 15\":\"Client Service Experts\", \"Unnamed: 16\":\"Client Service Set\", \"Unnamed: 17\":\"Sales\", \"Unnamed: 18\":\"Sales Experts\", \"Unnamed: 19\":\"Service Desk\", \"Unnamed: 20\":\"Underwriting\", \"Unnamed: 21\":\"Unite\", \"Unnamed: 22\":\"Unspecified\", \"Unnamed: 23\":\"Workforce\"}, inplace=True)\n",
    "CAH_volume_df.drop(index=0, inplace=True)\n",
    "     # Deletes columns for units that no longer exist or that we do not forecast for\n",
    "CAH_volume_df.drop([\"Advisor\", \"Agency Helpline\", \"ASU\", \"ASU Set\", \"Claims Back Office\", \"Claims Hertz\", \"Claims Lead Line\", \"Claims Material Damage\", \"Claims Service Center\", \"Claims Team Lead\", \"Client Service Set\", \"Service Desk\", \"Underwriting\", \"Unite\", \"Unspecified\", \"Workforce\"], axis = 1, inplace = True)\n",
    "     # Combines Sales and Service Experts, since they are forecasted as one unit\n",
    "CAH_volume_df[\"Experts\"] = CAH_volume_df[\"Sales Experts\"] + CAH_volume_df[\"Client Service Experts\"]\n",
    "CAH_volume_df.drop([\"Sales Experts\", \"Client Service Experts\"], axis = 1, inplace = True)\n",
    "    # Changes the Date column to objects instead of strings and adds a Year column\n",
    "CAH_volume_df[\"Date\"] = pd.to_datetime(CAH_volume_df.Date)\n",
    "CAH_volume_df[\"Year\"] = CAH_volume_df[\"Date\"].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe from the Define Matching Weeks excel file and cleans data\n",
    "define_matching_weeks_df = pd.read_excel(\"Define Matching Weeks.xlsx\")\n",
    "define_matching_weeks_df.fillna(\"none\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines SQL queries\n",
    "cisco_sql = \"\"\"\n",
    "Select cti.DateTime, cti.PrecisionQueueID, cti.ODSDataSourceID, dpq.EnterpriseName, dpq.Dept_Name, CallsOfferedRouted + CallsRequeried as CallsOffered\n",
    "From AcqCiscoAW.Call_Type_SG_Interval cti\n",
    "left join ArcCiscoAW.V_CallDataCisco_Dim_Precision_Queue dpq\n",
    "\ton REPLACE(dpq.PrecisionQueueID, '~', '') = concat(cti.PrecisionQueueID, ODSDataSourceID)\n",
    "Where cast(cti.DateTime as time) > '07:00:01'\n",
    "And cast(cti.Datetime as time) < '21:59:59'\n",
    "And cti.DateTime > ?\n",
    "And cti.DateTime < ?\n",
    "\"\"\"\n",
    "\n",
    "cisco_year_sql = '''Select sum(CallsOfferedRouted + CallsRequeried) as CallsOffered\n",
    "  From AcqCiscoAW.Call_Type_SG_Interval cti\n",
    "  left join ArcCiscoAW.V_CallDataCisco_Dim_Precision_Queue dpq\n",
    "  on REPLACE(dpq.PrecisionQueueID, '~', '') = concat(cti.PrecisionQueueID, ODSDataSourceID)\n",
    "  Where cast(cti.DateTime as date) > '2018-02-28'\n",
    "  And cast(cti.DateTime as time) > '07:00:01'\n",
    "  And cast(cti.Datetime as time) < '21:59:59'\n",
    "  and datepart(year, cast(cti.Datetime as date)) = ?\n",
    "  And dpq.Dept_Name = ?\n",
    "  '''\n",
    "\n",
    "cisco_by_date_sql =\"\"\"\n",
    "Select cti.DateTime as Date, CallsOfferedRouted + CallsRequeried as CallsOffered\n",
    "From AcqCiscoAW.Call_Type_SG_Interval cti\n",
    "left join ArcCiscoAW.V_CallData Cisco_Dim_Precision_Queue dpq\n",
    "\ton REPLACE(dpq.PrecisionQueueID, '~', '') = concat(cti.PrecisionQueueID, ODSDataSourceID)\n",
    "Where cast(cti.DateTime as time) > '07:00:01'\n",
    "And cast(cti.Datetime as time) < '21:59:59'\n",
    "And cti.DateTime > '2018-02-28'\n",
    "And dpq.Dept_Name = ?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines helper functions\n",
    "\n",
    "def str_to_object(str_date):\n",
    "    \"\"\"\n",
    "    Returns the date as a datetime object when entered in mm/dd/yyyy format.\n",
    "    \"\"\"\n",
    "    month = str_date[:2]\n",
    "    day = str_date[3:5]\n",
    "    year = str_date[-4:]\n",
    "    \n",
    "    iso_date = year + '-' + month + '-' + day\n",
    "    \n",
    "    output = dt.date.fromisoformat(iso_date)\n",
    "    return output\n",
    "\n",
    "\n",
    "def object_to_str(object_date):\n",
    "    \"\"\"\n",
    "    Returns the date in mm/dd/yyyy format when entered as a datetime object.\n",
    "    \"\"\"\n",
    "    iso_date = dt.date.isoformat(object_date)\n",
    "    \n",
    "    month = iso_date[5:7]\n",
    "    day = iso_date[-2:]\n",
    "    year = iso_date[:4]\n",
    "    \n",
    "    output = month + \"/\" + day + \"/\" + year\n",
    "    return output\n",
    "\n",
    "\n",
    "def to_iso(date):\n",
    "    \"\"\"\n",
    "    Returns the date in iso format when entered in mm/dd/yyyy format.\n",
    "    \"\"\"\n",
    "    month = date[:2]\n",
    "    day = date[3:5]\n",
    "    year = date[-4:]\n",
    "    \n",
    "    iso_date = year + '-' + month + '-' + day\n",
    "    \n",
    "    return iso_date\n",
    "\n",
    "\n",
    "def start_of_week(date, week_offset=0):\n",
    "    \"\"\"\n",
    "    Returns the Start of Week for the date when entered in mm/dd/yyyy format. If a Sunday is entered, returns the Sunday \\\n",
    "    prior. Week offset is the number of weeks in the future (positive number) or past (negative number).\n",
    "    \"\"\"\n",
    "    if type(date) == str:\n",
    "        dt_date = str_to_object(date)\n",
    "    else:\n",
    "        dt_date = date\n",
    "        \n",
    "    sow_week_prior = 0\n",
    "    \n",
    "    if dt.date.weekday(dt_date) == 0:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-1 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 1:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-2 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 2:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-3 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 3:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-4 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 4:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-5 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 5:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-6 + week_offset * 7)\n",
    "    elif dt.date.weekday(dt_date) == 6:\n",
    "        sow_week_prior = dt_date + dt.timedelta(days=-7 + week_offset * 7)\n",
    "        \n",
    "    output = object_to_str(sow_week_prior)\n",
    "    return output\n",
    "\n",
    "\n",
    "def day_volume(opco, date, unit):\n",
    "    \"\"\"Retrieves a single day's volume for a unit. Date must be in mm/dd/yyyy format.\"\"\"\n",
    "    \n",
    "    date = str_to_object(date)\n",
    "\n",
    "    global data_opco\n",
    "    global data_unit\n",
    "    global master_volume_dict\n",
    "    global queries_saved\n",
    "    global cursor\n",
    "\n",
    "    if unit != data_unit or opco != data_opco:\n",
    "        data_unit = unit\n",
    "        data_opco = opco\n",
    "        master_volume_dict = {}\n",
    "    else:\n",
    "        if date in master_volume_dict:\n",
    "            queries_saved += 1\n",
    "            return master_volume_dict[date]\n",
    "\n",
    "    if opco == \"CAH\":\n",
    "        \n",
    "        if date >= dt.date(2023, 1, 1):\n",
    "            if unit == \"Experts\":\n",
    "                return day_volume(object_to_str(date), \"Sales Experts\") + day_volume(object_to_str(date), \"Client Service Experts\")\n",
    "            date = to_iso(object_to_str(date))\n",
    "            cursor.execute(cisco_sql, date + ' 07:00:01', date + ' 21:59:59')\n",
    "            term_list = cursor.fetchall()\n",
    "            df = pd.DataFrame.from_records(term_list, columns=[col[0] for col in cursor.description])\n",
    "            output = int(df.loc[df[\"Dept_Name\"] == unit, \"CallsOffered\"].sum())\n",
    "            master_volume_dict[date] = output\n",
    "            return output\n",
    "        else:\n",
    "            temp_df1 = CAH_volume_df.loc[CAH_volume_df[\"Date\"] == to_iso(object_to_str(date))]\n",
    "            output = temp_df1[unit].sum()\n",
    "            master_volume_dict[date] = output\n",
    "            return output\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def week_volume(opco, start_of_week, unit):\n",
    "    \"\"\"Given a Sunday in mm/dd/yyyy format, retreives volume for that week in dictionary format.\"\"\"\n",
    "    start_of_week = str_to_object(start_of_week)\n",
    "    volume_dict = {}\n",
    "    for dow in range(7):\n",
    "        volume = day_volume(opco, object_to_str(start_of_week + pd.Timedelta(days=1 + dow)), unit)\n",
    "        if volume == 0:\n",
    "            continue\n",
    "        volume_dict[object_to_str(start_of_week + pd.Timedelta(days=1 + dow))] = volume\n",
    "    return volume_dict\n",
    "\n",
    "\n",
    "def dataframe(opco, unit):\n",
    "    \"\"\"Returns a dataframe with all daily volume data for a unit.\"\"\"\n",
    "    if opco == \"CAH\":\n",
    "        conn = pyodbc.connect('DRIVER={SQL Server};SERVER=aahssdbods.amfam.com;DATABASE=OperationalDataStore;Trusted_Connection=yes')\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(cisco_by_date_sql, unit)\n",
    "        term_list = cursor.fetchall()\n",
    "        df = pd.DataFrame.from_records(term_list, columns=[col[0] for col in cursor.description])\n",
    "        df[\"Date\"] = df[\"Date\"].dt.date\n",
    "        df = df.groupby([\"Date\"], as_index=False).sum()\n",
    "        df.rename(columns={\"Date\": \"ds\", \"CallsOffered\": \"y\"}, inplace=True)\n",
    "        return df\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "\n",
    "def total_volume_in_range(opco, start_date, end_date, unit):\n",
    "    \"\"\"Given start and end dates in mm/dd/yyyy format, returns total volume in that range. (end date excluded) (currently only works for dates after 3/1/2018)\"\"\"\n",
    "    if opco == \"CAH\":\n",
    "        conn = pyodbc.connect('DRIVER={SQL Server};SERVER=aahssdbods.amfam.com;DATABASE=OperationalDataStore;Trusted_Connection=yes')\n",
    "        cursor = conn.cursor()\n",
    "        if unit == \"Experts\":\n",
    "            return total_volume_in_range(start_date, end_date, \"Sales Experts\") + total_volume_in_range(start_date, end_date, \"Client Service Experts\")\n",
    "        \n",
    "        start_date = to_iso(start_date)\n",
    "        end_date = to_iso(end_date)\n",
    "        cursor.execute(cisco_sql, start_date + ' 00:00:00', end_date + ' 00:00:00')\n",
    "        term_list = cursor.fetchall()\n",
    "        df = pd.DataFrame.from_records(term_list, columns=[col[0] for col in cursor.description])\n",
    "        return int(df.loc[df[\"Dept_Name\"] == unit, \"CallsOffered\"].sum())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def yearly_volume(opco, year, unit):\n",
    "  \"\"\"Given a year entered as an interval, returns the total volume for that year.\"\"\"\n",
    "  if opco == \"CAH\":\n",
    "    conn = pyodbc.connect('DRIVER={SQL Server};SERVER=aahssdbods.amfam.com;DATABASE=OperationalDataStore;Trusted_Connection=yes')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if year <= 2023:\n",
    "        temp_df1 = CAH_volume_df.loc[CAH_volume_df[\"Year\"] == str(year)]\n",
    "        return temp_df1[unit].sum()\n",
    "    else:\n",
    "        cursor.execute(cisco_year_sql, str(year), unit)\n",
    "        term_list = cursor.fetchall()\n",
    "        df = pd.DataFrame.from_records(term_list, columns=[col[0] for col in cursor.description])\n",
    "        return df.CallsOffered.sum()\n",
    "  else:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(opco, unit, start_date=start_of_week((object_to_str(today)),1)):\n",
    "    \"\"\"Gathers relevant historical data to complete methods 1-4\"\"\"\n",
    "\n",
    "    global data_date\n",
    "    global data_dict\n",
    "    global data_opco\n",
    "    global yearly_volume_dict\n",
    "    global master_volume_dict\n",
    "\n",
    "    if opco == \"CAH\":\n",
    "        conn = pyodbc.connect('DRIVER={SQL Server};SERVER=aahssdbods.amfam.com;DATABASE=OperationalDataStore;Trusted_Connection=yes')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "    if data_dict == None or data_date != start_date:\n",
    "\n",
    "        print (\"Gathering relevant historical data...\")\n",
    "\n",
    "        data_dict = {}\n",
    "        data_date = start_date\n",
    "        \n",
    "        \n",
    "        data_dict[\"yearly_volume_dict\"] = {} #Dictionary of each year's total volume\n",
    "        data_dict[\"working_matching_weeks_list\"] = []  #Lists the start_date and the start of all matching weeks for prior years\n",
    "        data_dict[\"current_matching_weeks_list\"] = [] #Lists the start of last week and all matching weeks for prior years (used for year_output when start_date is in the future)\n",
    "        data_dict[\"matching_weeks_total_dict\"] = {} #Dictionary of the total volume for the matching weeks of each year\n",
    "        data_dict[\"matching_weeks_per_dict\"] = {} #Dictionary of the percent of total year volume for the matching weeks of each year\n",
    "        data_dict[\"dow_per_dict\"] = {} #Dictionary of the dow % for each day of week for the matching weeks of each year\n",
    "        data_dict[\"last_4_matching_weeks_total_dict\"] = {} #Dictionary of the total volume for the prior 4 matching weeks of each year\n",
    "        data_dict[\"last_4_matching_weeks_per_dict\"] = {} #Dictionary of the percentage of total year volume for the prior 4 matching weeks of each year\n",
    "        data_dict[\"week_to_compare_historical_volume\"] = {} #Dictionary of total volume for each prior year matching the comparison week.\n",
    "        data_dict[\"working_week_historical_volume\"] = {} #Dictionary of total volume for each prior year matching the forecast week.\n",
    "        data_dict[\"wow_list\"] = [] #List of % change between prior years' comparison and working weeks.\n",
    "        data_dict[\"week_prior_volume\"] = {} #Dictionary of volume by dow for the comparison week.\n",
    "        data_dict[\"week_prior_total_volume\"] = 0 #Sum of prior week's total volume.\n",
    "        data_dict[\"per_last_4_matching_volume\"] = {} #Dictionary of the percentage of volume received in the last 4 comparison weeks of prior years.\n",
    "        data_dict[\"per_last_4_matching_volume\"] = {} #Dictionary of the percentage of volume received in the forecast week of prior years.\n",
    "        data_dict[\"past_years_holiday_dow\"] = {} #When there is a moving holiday, dictionary of the dow that holiday fell on each year\n",
    "        data_dict[\"working_year\"] = start_date[-4:]\n",
    "\n",
    "        if start_date < start_of_week((object_to_str(today)),1):\n",
    "            week_to_compare = start_of_week(start_date, -1)\n",
    "        else:\n",
    "            week_to_compare = start_of_week(object_to_str(today), -1)\n",
    "\n",
    "        comparison_year = week_to_compare[-4:]\n",
    "\n",
    "        data_dict[\"last_4_current_year_volume\"] = sum(week_volume(opco, week_to_compare, unit).values()) + sum(week_volume(opco, start_of_week(week_to_compare), unit).values())\\\n",
    "        + sum(week_volume(opco, start_of_week(week_to_compare, -1), unit).values()) + sum(week_volume(opco, start_of_week(week_to_compare, -2), unit).values())\n",
    "\n",
    "        # Filling in working_matching_weeks_list\n",
    "        working_week_matching_weeks_df = define_matching_weeks_df.copy()\n",
    "        for i in range(int((len(define_matching_weeks_df.columns) + 1)/2)):\n",
    "            working_week_matching_weeks_df.pop(working_week_matching_weeks_df.columns.values[0])\n",
    "        working_row = working_week_matching_weeks_df.index[working_week_matching_weeks_df[data_dict[\"working_year\"] + \" Date\"] == start_date][0]\n",
    "        data_dict[\"working_matching_weeks_list\"] = working_week_matching_weeks_df.values[working_row].tolist()\n",
    "\n",
    "        # Filling in current_matching_weeks_list\n",
    "        current_week_matching_weeks_df = define_matching_weeks_df.copy()\n",
    "        for i in range(int((len(define_matching_weeks_df.columns) + 1)/2)):\n",
    "            current_week_matching_weeks_df.pop(current_week_matching_weeks_df.columns.values[0])\n",
    "        current_row = current_week_matching_weeks_df.index[current_week_matching_weeks_df[current_year + \" Date\"] == start_of_week((object_to_str(today)),1)][0]\n",
    "        data_dict[\"current_matching_weeks_list\"] = current_week_matching_weeks_df.values[current_row].tolist()\n",
    "\n",
    "        # Filling in comparison_weeks (intermediary)\n",
    "        comparison_week_matching_weeks_df = define_matching_weeks_df.copy()\n",
    "        for i in range(int((len(define_matching_weeks_df.columns) + 1)/2)):\n",
    "            comparison_week_matching_weeks_df.pop(comparison_week_matching_weeks_df.columns.values[0])\n",
    "        comparison_row = comparison_week_matching_weeks_df.index[comparison_week_matching_weeks_df[comparison_year + \" Date\"] == week_to_compare][0]\n",
    "        comparison_weeks = comparison_week_matching_weeks_df.values[comparison_row].tolist()\n",
    "        for week in list(comparison_weeks):\n",
    "            if int(week[-4:]) >= int(comparison_year):\n",
    "                comparison_weeks.remove(week)\n",
    "\n",
    "        # Filling in yearly_volume_dict\n",
    "        if yearly_volume_dict == {}:\n",
    "            for year in range(2012, int(object_to_str(today)[-4:])+1):\n",
    "                data_dict[\"yearly_volume_dict\"][year] = yearly_volume(opco, year, unit)\n",
    "                yearly_volume_dict = data_dict[\"yearly_volume_dict\"]\n",
    "        else:\n",
    "            data_dict[\"yearly_volume_dict\"] = yearly_volume_dict\n",
    "\n",
    "        # Filling in last_4_matching_weeks_total_dict \n",
    "        if start_date < start_of_week(object_to_str(today)):\n",
    "            for start in data_dict[\"working_matching_weeks_list\"]:\n",
    "                year_current = start[-4:]\n",
    "                data_dict[\"last_4_matching_weeks_total_dict\"][year_current] = {start_of_week(start,-1): sum(week_volume(opco, start_of_week(start,-1), unit).values()),\n",
    "                start_of_week(start,-2): sum(week_volume(opco, start_of_week(start,-2), unit).values()), \n",
    "                start_of_week(start,-3): sum(week_volume(opco, start_of_week(start,-3), unit).values()),\n",
    "                start_of_week(start,-4): sum(week_volume(opco, start_of_week(start,-4), unit).values())}\n",
    "        else:\n",
    "            for start in data_dict[\"current_matching_weeks_list\"]:\n",
    "                year_current = start[-4:]\n",
    "                data_dict[\"last_4_matching_weeks_total_dict\"][year_current] = {start_of_week(start,-1): sum(week_volume(opco, start_of_week(start,-1), unit).values()),\n",
    "                start_of_week(start,-2): sum(week_volume(opco, start_of_week(start,-2), unit).values()), \n",
    "                start_of_week(start,-3): sum(week_volume(opco, start_of_week(start,-3), unit).values()),\n",
    "                start_of_week(start,-4): sum(week_volume(opco, start_of_week(start,-4), unit).values())}\n",
    "\n",
    "        # Filling in last_4_matching_weeks_per_dict\n",
    "        for year in data_dict[\"last_4_matching_weeks_total_dict\"]:\n",
    "            try:\n",
    "                year_int = int(year)\n",
    "                if data_dict[\"yearly_volume_dict\"][year_int] == 0:\n",
    "                    continue\n",
    "                data_dict[\"last_4_matching_weeks_per_dict\"][year] = sum(data_dict[\"last_4_matching_weeks_total_dict\"][year].values()) / data_dict[\"yearly_volume_dict\"][year_int]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Filling in data_dict[\"matching_weeks_total_dict\"]\n",
    "        for start in data_dict[\"working_matching_weeks_list\"]:\n",
    "            year_working = start[-4:]\n",
    "            data_dict[\"matching_weeks_total_dict\"][year_working] = sum(week_volume(opco, start_of_week(start, 1), unit).values())\n",
    "\n",
    "        # Filling in data_dict[\"matching_weeks_per_dict\"]\n",
    "        for year in data_dict[\"matching_weeks_total_dict\"]:\n",
    "            if int(year) not in data_dict[\"yearly_volume_dict\"]:\n",
    "                continue\n",
    "            elif data_dict[\"yearly_volume_dict\"][int(year)] == 0:\n",
    "                continue\n",
    "            year_int = int(year)\n",
    "            data_dict[\"matching_weeks_per_dict\"][year] = data_dict[\"matching_weeks_total_dict\"][year] / data_dict[\"yearly_volume_dict\"][year_int]\n",
    "\n",
    "        # Filling in data_dict[\"dow_per_dict\"]\n",
    "        for start in data_dict[\"working_matching_weeks_list\"]:\n",
    "            year_current = start[-4:]\n",
    "            if int(year_current) >= int(data_dict[\"working_year\"]):\n",
    "                continue\n",
    "            if year_current not in data_dict[\"matching_weeks_total_dict\"] or data_dict[\"matching_weeks_total_dict\"][year_current] == 0:\n",
    "                continue\n",
    "            if year_current not in data_dict[\"dow_per_dict\"]:\n",
    "                data_dict[\"dow_per_dict\"][year_current] = {}\n",
    "            for dow in range(0, 7):\n",
    "                date_current = object_to_str(str_to_object(start) + dt.timedelta(days=dow + 1))\n",
    "                dow_current = weekday_dict[dow]\n",
    "                date_current_volume = day_volume(opco, date_current, unit)\n",
    "                data_dict[\"dow_per_dict\"][year_current][dow_current] = date_current_volume / data_dict[\"matching_weeks_total_dict\"][year_current]\n",
    "\n",
    "\n",
    "        # Filling in data_dict[\"week_to_compare_historical_volume\"]\n",
    "        for week in comparison_weeks:\n",
    "            data_dict[\"week_to_compare_historical_volume\"][week] = {}\n",
    "            current_week = week_volume(opco, week, unit)\n",
    "            for day in current_week:\n",
    "                for dow in weekday_dict.keys():\n",
    "                    if str_to_object(day).weekday() == dow:\n",
    "                        data_dict[\"week_to_compare_historical_volume\"][week][weekday_dict[dow]] = current_week[day]\n",
    "\n",
    "        # Filling in data_dict[\"working_week_historical_volume\"]\n",
    "        for week in data_dict[\"working_matching_weeks_list\"]:\n",
    "            data_dict[\"working_week_historical_volume\"][week] = {}\n",
    "            current_week = week_volume(opco, week, unit)\n",
    "            for day in current_week:\n",
    "                for dow in weekday_dict.keys():\n",
    "                    if str_to_object(day).weekday() == dow:\n",
    "                        data_dict[\"working_week_historical_volume\"][week][weekday_dict[dow]] = current_week[day]\n",
    "\n",
    "        # Filling in data_dict[\"wow_list\"]\n",
    "        for start in data_dict[\"working_week_historical_volume\"]:\n",
    "            temp_working_year = start[-4:]\n",
    "            for start2 in data_dict[\"week_to_compare_historical_volume\"]:\n",
    "                temp_comparison_year = start2[-4:]\n",
    "                if temp_comparison_year == temp_working_year:\n",
    "                    data_dict[\"wow_list\"].append(sum(data_dict[\"working_week_historical_volume\"][start].values()) / sum(data_dict[\"week_to_compare_historical_volume\"][start2].values()))\n",
    "\n",
    "        # Filling in data_dict[\"week_prior_volume\"]\n",
    "        data_dict[\"week_prior_volume\"] = week_volume(opco, week_to_compare, unit)\n",
    "\n",
    "        # Filling in data_dict[\"week_prior_total_volume\"]\n",
    "        data_dict[\"week_prior_total_volume\"] = sum(data_dict[\"week_prior_volume\"].values())\n",
    "\n",
    "        # Filling in data_dict[\"per_last_4_matching_volume\"]\n",
    "        for week in comparison_weeks:\n",
    "            year = week[-4:]\n",
    "            if int(year) not in data_dict[\"yearly_volume_dict\"]:\n",
    "                continue\n",
    "            data_dict[\"per_last_4_matching_volume\"][week[-4:]] = (sum(week_volume(opco, week, unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])]) + (sum(week_volume(opco, start_of_week(week), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])])\\\n",
    "            + (sum(week_volume(opco, start_of_week(week, -1), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])]) + (sum(week_volume(opco, start_of_week(week, -2), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])])\n",
    "\n",
    "        # Filling in data_dict[\"per_last_4_matching_volume\"]\n",
    "        for week in data_dict[\"working_matching_weeks_list\"]:\n",
    "            year = week[-4:]\n",
    "            if int(year) not in data_dict[\"yearly_volume_dict\"]:\n",
    "                continue\n",
    "            data_dict[\"per_last_4_matching_volume\"][week[-4:]] = (sum(week_volume(opco, week, unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])]) + (sum(week_volume(opco, start_of_week(week), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])])\\\n",
    "            + (sum(week_volume(opco, start_of_week(week, -1), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])]) + (sum(week_volume(opco, start_of_week(week, -2), unit).values()) / data_dict[\"yearly_volume_dict\"][int(week[-4:])])\n",
    "            \n",
    "        # Filling in data_dict[\"past_years_holiday_dow\"]\n",
    "        week_df = define_matching_weeks_df[define_matching_weeks_df.isin([start_date]).any(axis=1)]\n",
    "        if week_df['Holiday'].isin(moving_holidays).any():\n",
    "            data_dict[\"holiday\"] = \"Moving holiday\"\n",
    "        elif week_df['Holiday'].isin(static_holidays).any():\n",
    "            data_dict[\"holiday\"] = \"Static holiday\"\n",
    "        else:\n",
    "            data_dict[\"holiday\"] = \"No holiday\"\n",
    "        \n",
    "        if data_dict[\"holiday\"] == \"Moving holiday\":\n",
    "            holiday_name = week_df['Holiday'].item()\n",
    "            for date, name in sorted(holidays.US(years=int(start[-4:])).items()):\n",
    "                if name == holiday_name:\n",
    "                    holiday_date = date\n",
    "                else:\n",
    "                    continue\n",
    "            for year in data_dict[\"matching_weeks_total_dict\"]:\n",
    "                data_dict[\"past_years_holiday_dow\"][year] = str_to_object(object_to_str(holiday_date)[:-4]+year).weekday()\n",
    "            for year in data_dict[\"past_years_holiday_dow\"]:\n",
    "                if data_dict[\"past_years_holiday_dow\"][year] != holiday_date.weekday():\n",
    "                    del data_dict[\"matching_weeks_per_dict\"][year]\n",
    "\n",
    "        return data_dict\n",
    "    \n",
    "    else:\n",
    "        return data_dict\n",
    "    \n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_1(opco, unit, start_date=start_of_week((object_to_str(today)),1)):\n",
    "    \"\"\"\n",
    "    Returns the total volume forecasted for the given unit for next week using method 1, forecasting yearly volume using the last 4 weeks method.\n",
    "    \"\"\"\n",
    "\n",
    "    data = gather_data(opco, unit, start_date)\n",
    "\n",
    "    print(\"Running Method 1 for week of \" + start_date)\n",
    "\n",
    "    ### DELETING KEYS IN DICTS WHERE THE VALUE IS 0 ###\n",
    "\n",
    "    for year in list(data[\"yearly_volume_dict\"]):\n",
    "        if data[\"yearly_volume_dict\"][year] == 0:\n",
    "            del data[\"yearly_volume_dict\"][year]\n",
    "            \n",
    "    for year in list(data[\"matching_weeks_total_dict\"]):\n",
    "        if data[\"matching_weeks_total_dict\"][year] == 0:\n",
    "            del data[\"matching_weeks_total_dict\"][year]\n",
    "\n",
    "\n",
    "      \n",
    "    ### REMOVING OUTLIERS AND CALCULATING END VALUES ###\n",
    "    outliers_excluded = 0\n",
    "\n",
    "    count_last_4_per = 0\n",
    "    total_last_4_per = 0\n",
    "    year_per_values = []\n",
    "    for year in data[\"last_4_matching_weeks_per_dict\"]:\n",
    "        year_per_values.append(list(data[\"last_4_matching_weeks_per_dict\"].values()))\n",
    "    year_per_Q1 = np.percentile(year_per_values, 25)\n",
    "    year_per_Q3 = np.percentile(year_per_values, 75)\n",
    "    year_per_IQR = year_per_Q3 - year_per_Q1\n",
    "    year_per_upper = year_per_Q3 + 1.5 * year_per_IQR\n",
    "    year_per_lower = year_per_Q1 - 1.5 * year_per_IQR\n",
    "    for year in list(data[\"last_4_matching_weeks_per_dict\"]):\n",
    "        if data[\"last_4_matching_weeks_per_dict\"][year] >= year_per_upper:\n",
    "            del data[\"last_4_matching_weeks_per_dict\"][year]\n",
    "            outliers_excluded += 1\n",
    "        elif data[\"last_4_matching_weeks_per_dict\"][year] <= year_per_lower:\n",
    "            del data[\"last_4_matching_weeks_per_dict\"][year]\n",
    "            outliers_excluded += 1\n",
    "    for year in data[\"last_4_matching_weeks_per_dict\"]:\n",
    "        if int(year) >= int(data[\"working_year\"]):\n",
    "            continue\n",
    "        count_last_4_per += 1\n",
    "        total_last_4_per += data[\"last_4_matching_weeks_per_dict\"][year]\n",
    "    avg_per_last_4 = total_last_4_per / count_last_4_per\n",
    "    last_4_current_year_total = 0\n",
    "    if data[\"working_year\"] not in data[\"last_4_matching_weeks_per_dict\"]:\n",
    "        year_output = data[\"yearly_volume_dict\"][int(data[\"working_year\"])-1]\n",
    "    else:\n",
    "        for year in data[\"last_4_matching_weeks_per_dict\"]:\n",
    "            if year == data[\"working_year\"]:\n",
    "                for start in data[\"last_4_matching_weeks_per_dict\"][year]:\n",
    "                    last_4_current_year_total += data[\"last_4_matching_weeks_per_dict\"][year][start]\n",
    "        year_output = round((1 / avg_per_last_4) * last_4_current_year_total)\n",
    "\n",
    "    \n",
    "    total_woy_per = 0\n",
    "    count_woy_per = 0\n",
    "    woy_values = []\n",
    "    for year in data[\"matching_weeks_per_dict\"]:\n",
    "        woy_values.append(list(data[\"matching_weeks_per_dict\"].values()))\n",
    "    woy_Q1 = np.percentile(woy_values, 25)\n",
    "    woy_Q3 = np.percentile(woy_values, 75)\n",
    "    woy_IQR = woy_Q3 - woy_Q1\n",
    "    woy_upper = woy_Q3 + 1.5 * woy_IQR\n",
    "    woy_lower = woy_Q1 - 1.5 * woy_IQR\n",
    "    for year in list(data[\"matching_weeks_per_dict\"]):\n",
    "        if data[\"matching_weeks_per_dict\"][year] >= woy_upper:\n",
    "            del data[\"matching_weeks_per_dict\"][year]\n",
    "            outliers_excluded += 1\n",
    "        elif data[\"matching_weeks_per_dict\"][year] <= woy_lower:\n",
    "            del data[\"matching_weeks_per_dict\"][year]\n",
    "            outliers_excluded += 1\n",
    "    for year in data[\"matching_weeks_per_dict\"]:\n",
    "        if int(year) >= int(data[\"working_year\"]):\n",
    "            continue\n",
    "        else:\n",
    "            total_woy_per += data[\"matching_weeks_per_dict\"][year]\n",
    "            count_woy_per += 1\n",
    "    avg_woy_per = total_woy_per / count_woy_per\n",
    "    woy_output = round(avg_woy_per * year_output)\n",
    "\n",
    "    \n",
    "\n",
    "    dow_buckets = {}\n",
    "    for year in data[\"dow_per_dict\"]:\n",
    "        for dow in data[\"dow_per_dict\"][year]:\n",
    "            if dow not in dow_buckets:\n",
    "                dow_buckets[dow] = {}\n",
    "            dow_buckets[dow][year] = data[\"dow_per_dict\"][year][dow]\n",
    "\n",
    "\n",
    "    for dow in dow_buckets:\n",
    "        dow_values = []\n",
    "        for year in dow_buckets[dow]:\n",
    "            dow_values.append(list(dow_buckets[dow].values()))\n",
    "\n",
    "        dow_Q1 = np.percentile(dow_values, 25)\n",
    "        dow_Q3 = np.percentile(dow_values, 75)\n",
    "        dow_IQR = dow_Q3 - dow_Q1\n",
    "        dow_upper = dow_Q3 + 1.5 * dow_IQR\n",
    "        dow_lower = dow_Q1 - 1.5 * dow_IQR\n",
    "\n",
    "        for year in list(data[\"dow_per_dict\"]):\n",
    "            for dow2 in list(data[\"dow_per_dict\"][year]):\n",
    "                if dow2 == dow and data[\"dow_per_dict\"][year][dow] >= dow_upper:\n",
    "                    del data[\"dow_per_dict\"][year][dow]\n",
    "                    outliers_excluded += 1\n",
    "                elif dow2 == dow and data[\"dow_per_dict\"][year][dow] <= dow_lower:\n",
    "                    del data[\"dow_per_dict\"][year][dow]\n",
    "                    outliers_excluded += 1\n",
    "                    \n",
    "    \n",
    "    avg_dow_per = {\"Monday\": 0, \"Tuesday\": 0, \"Wednesday\": 0, \"Thursday\": 0, \"Friday\": 0, \"Saturday\": 0, \"Sunday\": 0}\n",
    "    \n",
    "    for dow in avg_dow_per.keys():\n",
    "        count_dow_per = 0\n",
    "        total_dow_per = 0\n",
    "        for year in data[\"dow_per_dict\"]:\n",
    "            for dow2 in data[\"dow_per_dict\"][year]:\n",
    "                if dow2 == dow:\n",
    "                    total_dow_per += data[\"dow_per_dict\"][year][dow]\n",
    "                    count_dow_per += 1\n",
    "        if count_dow_per == 0:\n",
    "            avg_dow_per[dow] = 0\n",
    "        else:\n",
    "            avg_dow_per[dow] = total_dow_per / count_dow_per\n",
    "            \n",
    "    dow_output = {}\n",
    "\n",
    "    \n",
    "    for dow in avg_dow_per.keys():\n",
    "        if avg_dow_per[dow] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dow_output[dow] = round(woy_output * avg_dow_per[dow])\n",
    "            \n",
    "\n",
    "    return {\"start_date\": start_date, \"unit\": unit, \"forecast\": dow_output, \"outliers_excluded\": outliers_excluded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_2(opco, unit, start_date=start_of_week((object_to_str(today)),1)):\n",
    "    \"\"\"\n",
    "    Returns the total volume forecasted for the given unit for next week using method 2.\n",
    "    \"\"\"\n",
    "    data = gather_data(opco, unit, start_date)\n",
    "\n",
    "    print(\"Running Method 2 for week of \" + start_date)\n",
    "\n",
    "        ### REMOVING OUTLIERS AND CALCULATING END VALUES ###\n",
    "    outliers_excluded = 0\n",
    "\n",
    "    if data[\"holiday\"] != \"Moving Holiday\":\n",
    "        wow_Q1 = np.percentile(data[\"wow_list\"], 25)\n",
    "        wow_Q3 = np.percentile(data[\"wow_list\"], 75)\n",
    "        wow_IQR = wow_Q3 - wow_Q1\n",
    "        wow_upper = wow_Q3 + 1.5 * wow_IQR\n",
    "        wow_lower = wow_Q1 - 1.5 * wow_IQR\n",
    "\n",
    "        for wow in list(data[\"wow_list\"]):\n",
    "            if wow > wow_upper:\n",
    "                data[\"wow_list\"].remove(wow)\n",
    "                outliers_excluded += 1\n",
    "            elif wow < wow_lower:\n",
    "                data[\"wow_list\"].remove(wow)\n",
    "                outliers_excluded += 1\n",
    "                      \n",
    "    wow_output = sum(data[\"wow_list\"]) / len(data[\"wow_list\"])     \n",
    "       \n",
    "    if data[\"holiday\"] != \"Moving Holiday\":\n",
    "        dow_buckets = {}\n",
    "\n",
    "        for start in data[\"dow_per_dict\"]:\n",
    "            for dow in data[\"dow_per_dict\"][start]:\n",
    "                if dow not in dow_buckets:\n",
    "                    dow_buckets[dow] = {}\n",
    "                dow_buckets[dow][start[-4:]] = data[\"dow_per_dict\"][start][dow]\n",
    "\n",
    "        for dow in dow_buckets:\n",
    "            dow_values = []\n",
    "            for year in dow_buckets[dow]:\n",
    "                dow_values.append(list(dow_buckets[dow].values()))\n",
    "\n",
    "            dow_Q1 = np.percentile(dow_values, 25)\n",
    "            dow_Q3 = np.percentile(dow_values, 75)\n",
    "            dow_IQR = dow_Q3 - dow_Q1\n",
    "            dow_upper = dow_Q3 + 1.5 * dow_IQR\n",
    "            dow_lower = dow_Q1 - 1.5 * dow_IQR\n",
    "\n",
    "            for start in list(data[\"dow_per_dict\"]):\n",
    "                for dow2 in list(data[\"dow_per_dict\"][start]):\n",
    "                    if dow2 == dow and data[\"dow_per_dict\"][start][dow] >= dow_upper:\n",
    "                        del data[\"dow_per_dict\"][start][dow]\n",
    "                        outliers_excluded += 1\n",
    "                    elif dow2 == dow and data[\"dow_per_dict\"][start][dow] <= dow_lower:\n",
    "                        del data[\"dow_per_dict\"][start][dow]\n",
    "                        outliers_excluded += 1\n",
    "    \n",
    "    avg_dow_per = {\"Monday\": 0, \"Tuesday\": 0, \"Wednesday\": 0, \"Thursday\": 0, \"Friday\": 0, \"Saturday\": 0, \"Sunday\": 0}\n",
    "    for dow in avg_dow_per.keys():\n",
    "        count_dow_per = 0\n",
    "        total_dow_per = 0\n",
    "        for week in data[\"dow_per_dict\"]:\n",
    "            for dow2 in data[\"dow_per_dict\"][week]:\n",
    "                if dow2 == dow:\n",
    "                    total_dow_per += data[\"dow_per_dict\"][week][dow]\n",
    "                    count_dow_per += 1\n",
    "        if count_dow_per == 0:\n",
    "            avg_dow_per[dow] = 0\n",
    "        else:\n",
    "            avg_dow_per[dow] = total_dow_per / count_dow_per\n",
    "            \n",
    "    dow_output = {}\n",
    "    \n",
    "    for dow in avg_dow_per:\n",
    "        if avg_dow_per[dow] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dow_output[dow] = round(data_dict[\"week_prior_total_volume\"] * wow_output * avg_dow_per[dow])\n",
    "            \n",
    "    return {\"start_date\": start_date, \"unit\": unit, \"forecast\": dow_output, \"outliers_excluded\": outliers_excluded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_3(opco, unit, start_date=start_of_week((object_to_str(today)),1)):\n",
    "    \"\"\"\n",
    "    Returns the total volume forecasted for the given unit for next week using method 3.\n",
    "    \"\"\"\n",
    "    data = gather_data(opco, unit, start_date)\n",
    "\n",
    "    print(\"Running Method 3 for week of \" + start_date)\n",
    "\n",
    "     ### DELETING KEYS IN DICTS WHERE THE VALUE IS 0 ###               \n",
    "    for dow in list(data[\"week_prior_volume\"]):\n",
    "        if data[\"week_prior_volume\"][dow] == 0:\n",
    "            del data[\"week_prior_volume\"][dow]\n",
    "            \n",
    "    for week in list(data[\"week_to_compare_historical_volume\"]):\n",
    "        for dow in list(data[\"week_to_compare_historical_volume\"][week]):\n",
    "            if data[\"week_to_compare_historical_volume\"][week][dow] == 0:\n",
    "                del data[\"week_to_compare_historical_volume\"][week][dow]\n",
    "                \n",
    "    for week in list(data[\"working_week_historical_volume\"]):\n",
    "        for dow in list(data[\"working_week_historical_volume\"][week]):\n",
    "            if data[\"working_week_historical_volume\"][week][dow] == 0:\n",
    "                del data[\"working_week_historical_volume\"][week][dow]\n",
    "\n",
    "\n",
    "    ### REMOVING OUTLIERS AND CALCULATING END VALUES ###            \n",
    "    dow_buckets_week_to_compare_historical_volume = {}\n",
    "    for start in data[\"week_to_compare_historical_volume\"]:\n",
    "        for dow in data[\"week_to_compare_historical_volume\"][start]:\n",
    "            if dow not in dow_buckets_week_to_compare_historical_volume:\n",
    "                dow_buckets_week_to_compare_historical_volume[dow] = {}\n",
    "            dow_buckets_week_to_compare_historical_volume[dow][start[-4:]] = data[\"week_to_compare_historical_volume\"][start][dow]\n",
    "\n",
    "            \n",
    "    dow_buckets_working_week_historical_volume = {}\n",
    "    for start in data[\"working_week_historical_volume\"]:\n",
    "        for dow in data[\"working_week_historical_volume\"][start]:\n",
    "            if dow not in dow_buckets_working_week_historical_volume:\n",
    "                dow_buckets_working_week_historical_volume[dow] = {}\n",
    "            dow_buckets_working_week_historical_volume[dow][start[-4:]] = data[\"working_week_historical_volume\"][start][dow]\n",
    "            \n",
    "    per_change = {dow: {year: dow_buckets_working_week_historical_volume[dow][year]/dow_buckets_week_to_compare_historical_volume[dow][year] for year in dow_buckets_working_week_historical_volume[dow].keys() & dow_buckets_week_to_compare_historical_volume[dow]} for dow in dow_buckets_working_week_historical_volume.keys() & dow_buckets_week_to_compare_historical_volume}\n",
    "    for dow in list(per_change):\n",
    "        check_empty = not bool(per_change[dow])\n",
    "        if check_empty == True:\n",
    "            del per_change[dow]\n",
    "            \n",
    "\n",
    "    outliers_excluded = 0\n",
    "    \n",
    "    if data[\"holiday\"] != \"Moving Holiday\":\n",
    "\n",
    "        for dow in per_change:\n",
    "            dod_values = []\n",
    "            for year in dow_buckets_working_week_historical_volume[dow]:\n",
    "                dod_values.append(list(per_change[dow].values()))\n",
    "\n",
    "            dod_Q1 = np.percentile(dod_values, 25)\n",
    "            dod_Q3 = np.percentile(dod_values, 75)\n",
    "            dod_IQR = dod_Q3 - dod_Q1\n",
    "            dod_upper = dod_Q3 + 1.5 * dod_IQR\n",
    "            dod_lower = dod_Q1 - 1.5 * dod_IQR\n",
    "\n",
    "            for year in list(per_change[dow]):\n",
    "                if per_change[dow][year] > dod_upper:\n",
    "                    del per_change[dow][year]\n",
    "                    outliers_excluded += 1\n",
    "                elif per_change[dow][year] < dod_lower:\n",
    "                    del per_change[dow][year]\n",
    "                    outliers_excluded += 1\n",
    "\n",
    "    dod_output = {}\n",
    "    for dow in list(per_change):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        if len(per_change[dow]) == 0:\n",
    "            del per_change[dow]\n",
    "        else:\n",
    "            for year in per_change[dow]:\n",
    "                total += per_change[dow][year]\n",
    "                count += 1\n",
    "            dod_output[dow] = total / count\n",
    "\n",
    "        \n",
    "    forecast = {}\n",
    "    for date in data[\"week_prior_volume\"]:\n",
    "        dow = weekday_dict[dt.date.weekday(str_to_object(date))]\n",
    "        if dow not in dod_output:\n",
    "            continue\n",
    "        forecast[dow] = round(data[\"week_prior_volume\"][date] * dod_output[dow])\n",
    "    \n",
    "    return {\"start_date\": start_date, \"unit\": unit, \"forecast\": forecast, \"outliers_excluded\": outliers_excluded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_4(opco, unit, start_date=start_of_week((object_to_str(today)),1)):\n",
    "    \"\"\"\n",
    "    Returns the total volume forecasted for the given unit for next week using method 4.\n",
    "    \"\"\"\n",
    "    data = gather_data(opco, unit, start_date)\n",
    "\n",
    "    print(\"Running Method 4 for week of \" + start_date)\n",
    "\n",
    "    ### REMOVING OUTLIERS AND CALCULATING END VALUES ### \n",
    "    outliers_excluded = 0\n",
    "\n",
    "    for year in list(data[\"per_last_4_matching_volume\"]):\n",
    "        year_values = list(data[\"per_last_4_matching_volume\"].values())\n",
    "\n",
    "        year_Q1 = np.percentile(year_values, 25)\n",
    "        year_Q3 = np.percentile(year_values, 75)\n",
    "        year_IQR = year_Q3 - year_Q1\n",
    "        year_upper = year_Q3 + 1.5 * year_IQR\n",
    "        year_lower = year_Q1 - 1.5 * year_IQR\n",
    "\n",
    "        for year in list(data[\"per_last_4_matching_volume\"]):\n",
    "            if data[\"per_last_4_matching_volume\"][year] > year_upper:\n",
    "                del data[\"per_last_4_matching_volume\"][year]\n",
    "                outliers_excluded += 1\n",
    "            elif data[\"per_last_4_matching_volume\"][year] < year_lower:\n",
    "                del data[\"per_last_4_matching_volume\"][year]\n",
    "                outliers_excluded += 1\n",
    "    \n",
    "    avg_per_last_4 = sum(data[\"per_last_4_matching_volume\"].values()) / len(data[\"per_last_4_matching_volume\"]) / 4\n",
    "        \n",
    "\n",
    "    if data[\"holiday\"] != \"Moving Holiday\":\n",
    "        \n",
    "        for year in list(data[\"last_4_matching_weeks_per_dict\"]):\n",
    "            year_values = list(data[\"last_4_matching_weeks_per_dict\"].values())\n",
    "\n",
    "            year_Q1 = np.percentile(year_values, 25)\n",
    "            year_Q3 = np.percentile(year_values, 75)\n",
    "            year_IQR = year_Q3 - year_Q1\n",
    "            year_upper = year_Q3 + 1.5 * year_IQR\n",
    "            year_lower = year_Q1 - 1.5 * year_IQR\n",
    "\n",
    "            for year in list(data[\"last_4_matching_weeks_per_dict\"]):\n",
    "                if data[\"last_4_matching_weeks_per_dict\"][year] > year_upper:\n",
    "                    del data[\"last_4_matching_weeks_per_dict\"][year]\n",
    "                    outliers_excluded += 1\n",
    "                elif data[\"last_4_matching_weeks_per_dict\"][year] < year_lower:\n",
    "                    del data[\"last_4_matching_weeks_per_dict\"][year]\n",
    "                    outliers_excluded += 1\n",
    "                \n",
    "    avg_per_working = sum(data[\"last_4_matching_weeks_per_dict\"].values()) / len(data[\"last_4_matching_weeks_per_dict\"])\n",
    "    \n",
    "    per_change = (avg_per_working - avg_per_last_4)\n",
    "    \n",
    "    total_week_forecast = data[\"last_4_current_year_volume\"] * (1 + per_change) / 4\n",
    "\n",
    "        \n",
    "    dow_buckets = {}\n",
    "    \n",
    "    for week in data[\"dow_per_dict\"]:\n",
    "        for dow in data[\"dow_per_dict\"][week]:\n",
    "            if dow not in dow_buckets:\n",
    "                dow_buckets[dow] = []\n",
    "            dow_buckets[dow].append(data[\"dow_per_dict\"][week][dow])\n",
    "    \n",
    "    if data[\"holiday\"] != \"Moving Holiday\":\n",
    "\n",
    "        for dow in dow_buckets:\n",
    "            dow_Q1 = np.percentile(dow_buckets[dow], 25)\n",
    "            dow_Q3 = np.percentile(dow_buckets[dow], 75)\n",
    "            dow_IQR = dow_Q3 - dow_Q1\n",
    "            dow_upper = dow_Q3 + 1.5 * dow_IQR\n",
    "            dow_lower = dow_Q1 - 1.5 * dow_IQR\n",
    "\n",
    "            for value in list(dow_buckets[dow]):\n",
    "                if value > dow_upper:\n",
    "                    dow_buckets[dow].remove(value)\n",
    "                    outliers_excluded += 1\n",
    "                elif value < dow_lower:\n",
    "                    dow_buckets[dow].remove(value)\n",
    "                    outliers_excluded += 1\n",
    "                \n",
    "    dow_average = {}\n",
    "    \n",
    "    for dow in dow_buckets:\n",
    "        dow_average[dow] = sum(dow_buckets[dow]) / len(dow_buckets[dow])\n",
    "        \n",
    "    dow_output = {}\n",
    "    \n",
    "    for dow in dow_average:\n",
    "        if dow_average[dow] == 0:\n",
    "            continue\n",
    "        dow_output[dow] = round(dow_average[dow] * total_week_forecast)\n",
    "\n",
    "    return {\"start_date\": start_date, \"unit\": unit, \"forecast\": dow_output, \"outliers_excluded\": outliers_excluded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tactical_volume_forecast_v2(opco, unit):\n",
    "    \"\"\"\n",
    "    Returns the tactical (next 3 weeks) volume forecast using an elastic net model.\n",
    "    \"\"\"\n",
    "    global queries_saved\n",
    "    \n",
    "    forecast_week = object_to_str(today)\n",
    "    \n",
    "    accuracy_dict = {}\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    \n",
    "    current_week = -4\n",
    "    \n",
    "    while current_week < 0:\n",
    "        \n",
    "        if current_week == -4:\n",
    "            accuracy_dict[1] = {}\n",
    "            accuracy_dict[2] = {}\n",
    "            accuracy_dict[3] = {}\n",
    "            accuracy_dict[4] = {}\n",
    "        \n",
    "        current_method = 4\n",
    "    \n",
    "        while current_method > 0:\n",
    "            \n",
    "\n",
    "            if current_method == 1:\n",
    "                temp_forecast_dict = method_1(opco, unit, start_of_week(forecast_week, current_week))\n",
    "            elif current_method == 2:\n",
    "                temp_forecast_dict = method_2(opco, unit, start_of_week(forecast_week, current_week))\n",
    "            elif current_method == 3:\n",
    "                temp_forecast_dict = method_3(opco, unit, start_of_week(forecast_week, current_week))\n",
    "            elif current_method == 4:\n",
    "                temp_forecast_dict = method_4(opco, unit, start_of_week(forecast_week, current_week))\n",
    "\n",
    "            for dow in temp_forecast_dict[\"forecast\"]:\n",
    "                if dow == \"Monday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=1))] = int(temp_forecast_dict[\"forecast\"][\"Monday\"])\n",
    "                elif dow == \"Tuesday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=2))] = int(temp_forecast_dict[\"forecast\"][\"Tuesday\"])\n",
    "                elif dow == \"Wednesday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=3))] = int(temp_forecast_dict[\"forecast\"][\"Wednesday\"])\n",
    "                elif dow == \"Thursday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=4))] = int(temp_forecast_dict[\"forecast\"][\"Thursday\"])\n",
    "                elif dow == \"Friday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=5))] = int(temp_forecast_dict[\"forecast\"][\"Friday\"])\n",
    "                elif dow == \"Saturday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=6))] = int(temp_forecast_dict[\"forecast\"][\"Saturday\"])\n",
    "                elif dow == \"Saturday\":\n",
    "                    accuracy_dict[current_method][object_to_str(str_to_object(temp_forecast_dict[\"start_date\"]) + dt.timedelta(days=7))] = int(temp_forecast_dict[\"forecast\"][\"Sunday\"])\n",
    "\n",
    "            current_method = current_method - 1\n",
    "        \n",
    "        current_week = current_week + 1\n",
    "    \n",
    "    df = pd.DataFrame(accuracy_dict)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    actuals_dict = {}\n",
    "    \n",
    "    for date in accuracy_dict[1]:\n",
    "        actuals_dict[date] = day_volume(opco, date, unit)\n",
    "        \n",
    "    df[\"Actual\"] = pd.Series(actuals_dict)\n",
    "    \n",
    "    X = df[[1,2,3,4]]\n",
    "    y = df['Actual']\n",
    "    \n",
    "    elastic = ElasticNetCV()\n",
    "    elastic.fit(X,y)\n",
    "    \n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X, y)\n",
    "\n",
    "    print(\"Forecasting...\")\n",
    "    \n",
    "    week1method1 = method_1(opco, unit, start_of_week(forecast_week, 1))\n",
    "    week1method2 = method_2(opco, unit, start_of_week(forecast_week, 1))\n",
    "    week1method3 = method_3(opco, unit, start_of_week(forecast_week, 1))\n",
    "    week1method4 = method_4(opco, unit, start_of_week(forecast_week, 1))\n",
    "    week2method1 = method_1(opco, unit, start_of_week(forecast_week, 2))\n",
    "    week2method2 = method_2(opco, unit, start_of_week(forecast_week, 2))\n",
    "    week2method3 = method_3(opco, unit, start_of_week(forecast_week, 2))\n",
    "    week2method4 = method_4(opco, unit, start_of_week(forecast_week, 2))\n",
    "    week3method1 = method_1(opco, unit, start_of_week(forecast_week, 3))\n",
    "    week3method2 = method_2(opco, unit, start_of_week(forecast_week, 3))\n",
    "    week3method3 = method_3(opco, unit, start_of_week(forecast_week, 3))\n",
    "    week3method4 = method_4(opco, unit, start_of_week(forecast_week, 3))\n",
    "    \n",
    "    for day in week1method1[\"forecast\"]:\n",
    "        if day not in week1method3:\n",
    "            week1method3[\"forecast\"][day] = week1method2[\"forecast\"][day]\n",
    "    for day in week2method1[\"forecast\"]:\n",
    "        if day not in week2method3:\n",
    "            week2method3[\"forecast\"][day] = week2method2[\"forecast\"][day]\n",
    "    for day in week3method1[\"forecast\"]:\n",
    "        if day not in week3method3:\n",
    "            week3method3[\"forecast\"][day] = week3method2[\"forecast\"][day]\n",
    "    \n",
    "\n",
    "\n",
    "    final_answer = {\"unit\": unit, start_of_week(forecast_week, 1):{}, start_of_week(forecast_week, 2):{}, start_of_week(forecast_week, 3):{}}\n",
    "\n",
    "    #WEEK 1 FINAL ANSWER\n",
    "    week_df = define_matching_weeks_df[define_matching_weeks_df.isin([start_of_week(forecast_week,1)]).any(axis=1)]\n",
    "    if week_df['Holiday'].isin(moving_holidays).any():\n",
    "        holiday = \"Moving Holiday\"\n",
    "    elif week_df['Holiday'].isin(static_holidays).any():\n",
    "        holiday = \"Static Holiday\"\n",
    "    else:\n",
    "        holiday = \"No Holiday\"\n",
    "    print(holiday)\n",
    "    for day in week1method1[\"forecast\"]:\n",
    "        predict = elastic.predict([[week1method1[\"forecast\"][day],week1method2[\"forecast\"][day],week1method3[\"forecast\"][day],week1method4[\"forecast\"][day]]])\n",
    "        final_answer[start_of_week(forecast_week, 1)][day] = int(predict.item())\n",
    "\n",
    "    #WEEK 2 FINAL ANSWER\n",
    "    week_df = define_matching_weeks_df[define_matching_weeks_df.isin([start_of_week(forecast_week,2)]).any(axis=1)]\n",
    "    if week_df['Holiday'].isin(moving_holidays).any():\n",
    "        holiday = \"Moving Holiday\"\n",
    "    elif week_df['Holiday'].isin(static_holidays).any():\n",
    "        holiday = \"Static Holiday\"\n",
    "    else:\n",
    "        holiday = \"No Holiday\"\n",
    "    print(holiday)\n",
    "    for day in week2method1[\"forecast\"]:\n",
    "        predict = elastic.predict([[week2method1[\"forecast\"][day],week2method2[\"forecast\"][day],week2method3[\"forecast\"][day],week2method4[\"forecast\"][day]]])  \n",
    "        final_answer[start_of_week(forecast_week, 2)][day] = int(predict.item())\n",
    "\n",
    "    #WEEK 3 FINAL ANSWER\n",
    "    week_df = define_matching_weeks_df[define_matching_weeks_df.isin([start_of_week(forecast_week,3)]).any(axis=1)]\n",
    "    if week_df['Holiday'].isin(moving_holidays).any():\n",
    "        holiday = \"Moving Holiday\"\n",
    "    elif week_df['Holiday'].isin(static_holidays).any():\n",
    "        holiday = \"Static Holiday\"\n",
    "    else:\n",
    "        holiday = \"No Holiday\"\n",
    "    print(holiday)\n",
    "    if week3method1 == week3method2 == week3method3 == week3method4:\n",
    "        for day in week3method1[\"forecast\"]:\n",
    "            predict = regr.predict([[week3method1[\"forecast\"][day],week3method2[\"forecast\"][day],week3method3[\"forecast\"][day],week3method4[\"forecast\"][day]]])\n",
    "            final_answer[start_of_week(forecast_week, 3)][day] = int(predict.item())\n",
    "    else:\n",
    "        for day in week3method1[\"forecast\"]:\n",
    "            predict = elastic.predict([[week3method1[\"forecast\"][day],week3method2[\"forecast\"][day],week3method3[\"forecast\"][day],week3method4[\"forecast\"][day]]])\n",
    "            final_answer[start_of_week(forecast_week, 3)][day] = int(predict.item())\n",
    "\n",
    "    print(queries_saved)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Gathering relevant historical data...\n",
      "Running Method 4 for week of 12/10/2023\n",
      "Running Method 3 for week of 12/10/2023\n",
      "Running Method 2 for week of 12/10/2023\n",
      "Running Method 1 for week of 12/10/2023\n",
      "Gathering relevant historical data...\n",
      "Running Method 4 for week of 12/17/2023\n",
      "Running Method 3 for week of 12/17/2023\n",
      "Running Method 2 for week of 12/17/2023\n",
      "Running Method 1 for week of 12/17/2023\n",
      "Gathering relevant historical data...\n",
      "Running Method 4 for week of 12/24/2023\n",
      "Running Method 3 for week of 12/24/2023\n",
      "Running Method 2 for week of 12/24/2023\n",
      "Running Method 1 for week of 12/24/2023\n",
      "Gathering relevant historical data...\n",
      "Running Method 4 for week of 12/31/2023\n",
      "Running Method 3 for week of 12/31/2023\n",
      "Running Method 2 for week of 12/31/2023\n",
      "Running Method 1 for week of 12/31/2023\n",
      "Forecasting...\n",
      "Gathering relevant historical data...\n",
      "Running Method 1 for week of 01/14/2024\n",
      "Running Method 2 for week of 01/14/2024\n",
      "Running Method 3 for week of 01/14/2024\n",
      "Running Method 4 for week of 01/14/2024\n",
      "Gathering relevant historical data...\n",
      "Running Method 1 for week of 01/21/2024\n",
      "Running Method 2 for week of 01/21/2024\n",
      "Running Method 3 for week of 01/21/2024\n",
      "Running Method 4 for week of 01/21/2024\n",
      "Gathering relevant historical data...\n",
      "Running Method 1 for week of 01/28/2024\n",
      "Running Method 2 for week of 01/28/2024\n",
      "Running Method 3 for week of 01/28/2024\n",
      "Running Method 4 for week of 01/28/2024\n",
      "Static Holiday\n",
      "No Holiday\n",
      "No Holiday\n",
      "7843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unit': 'Sales',\n",
       " '01/14/2024': {'Monday': 984,\n",
       "  'Tuesday': 1108,\n",
       "  'Wednesday': 1092,\n",
       "  'Thursday': 1036,\n",
       "  'Friday': 975,\n",
       "  'Saturday': 453},\n",
       " '01/21/2024': {'Monday': 1212,\n",
       "  'Tuesday': 1056,\n",
       "  'Wednesday': 981,\n",
       "  'Thursday': 908,\n",
       "  'Friday': 911,\n",
       "  'Saturday': 424},\n",
       " '01/28/2024': {'Monday': 1258,\n",
       "  'Tuesday': 1072,\n",
       "  'Wednesday': 992,\n",
       "  'Thursday': 919,\n",
       "  'Friday': 902,\n",
       "  'Saturday': 418}}"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tactical_volume_forecast_v2(\"CAH\", \"Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
